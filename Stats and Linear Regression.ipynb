{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science from Scratch Tutorial\n",
    "## Statistics and Linear Regression\n",
    "### Gail Muldoon 2016\n",
    "\n",
    "This notebook uses titanic passengers and cricket chirp datasets to cover Statistics (Chapter 5) and Linear Regression (Chapter 14) from the book Data Science From Scratch by Joel Grus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load library dependencies and document software versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import pandas\n",
    "import pandas as pd\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#Import statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.graphics.api as smg\n",
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Import seaborn\n",
    "import seaborn as sns \n",
    "sns.set_style(\"white\")\n",
    "#make figures appear as they're made\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Always good practice to note the version info\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "print(\"python \" + sys.version)\n",
    "print(\"\")\n",
    "print(\"pandas \" + str(pd.__version__))\n",
    "print(\"numpy \" + np.__version__)\n",
    "#print(\"matplotlib \" + mpl.__version__)\n",
    "print(\"seaborn \" + sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/titanic_train.csv\")\n",
    "print(titanic.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistic is just a number that describes some aspect of data. There are many basic stats that might be of interest for a dataset:\n",
    "\n",
    "1. Central Tendencies: where is the data centered?\n",
    "    * mean\n",
    "    * median\n",
    "    * mode\n",
    "    * quantiles\n",
    "\n",
    "2. Disperson: spread of data\n",
    "    * range (max - min)\n",
    "    * interquartile range (75th percentile - 25th percentile)\n",
    "    * standard deviation / variance\n",
    "    * outliers\n",
    "    \n",
    "3. Correlation: how do variables relate to one another?\n",
    "    * covariance\n",
    "    * correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Descriptive statistics of dataset\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Why does Age have a lower \"count\" than the other variables?\n",
    "#### Question 2: Why are variables like Sex not shown in .describe()?\n",
    "\n",
    "### More central tendencies\n",
    "Might also be interested in median (the center value), which is not sensitive to outliers like the mean is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get median for all numeric variables\n",
    "titanic.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It doesn't always make sense to take mean/median of a variable just because it's numeric\n",
    "i.e. median of PassengerId doesn't have meaning\n",
    "\n",
    "Or the mode, which is the value(s) that appear most often for a variable, whether they be numeric or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More dispersion\n",
    "Can also get things like highest or lowest N values for a variable, like fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicSorted = sorted(titanic.Fare)\n",
    "\n",
    "#10 lowest fares (i.e. the first 10 after sorting)\n",
    "print  titanicSorted[:10]\n",
    "\n",
    "#10 highest fares (i.e. the last 10 after sorting)\n",
    "print titanicSorted[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or calculate arbitrary quantiles (i.e. percentiles):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute 10% quantify, value for which 10% of data lies below\n",
    "print(titanic.quantile(0.1))\n",
    "print('')\n",
    "# or 83.4% quantile, just in case\n",
    "print(titanic.quantile(0.834))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Choose your own quantile to calculate and interpret what it means.\n",
    "\n",
    "Variance is another measure of dispersion which is just the squared standard deviation. Keep in mind standard deviation has the same units as the variable being measured, while the variance has units of the square of the value being measured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here's the variance\n",
    "print(titanic.var())\n",
    "\n",
    "#computing from standard deviation to check\n",
    "var=titanic.std()**2\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures of dispersion tend to be sensitive to outliers. One measure of dispersion that's less sensitive to outliers is the interquartile range, which is the range between the 25th and 75th percentiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute interquartile range\n",
    "interquartile = titanic.quantile(0.75) - titanic.quantile(0.25)\n",
    "print(interquartile)\n",
    "\n",
    "#compute the regular range for comparison\n",
    "#Note range is already function in python, so I call it Range\n",
    "Range = max(titanic.Fare) - min(titanic.Fare)\n",
    "print(Range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance \n",
    "From the book: \"Whereas variance measures how a single variable deviates from its mean, covariance measures how two variables vary in tandem from their means.\" \n",
    "\n",
    "If the covariance is positive and relatively large, it means if one variable is above its mean value, the other variable tends to be above its mean or vice versa (both below their respective means). A large negative covariance indicates if one variable is above its mean, the other variable tends to be below its mean. Covariance close to 0 indicates no relationship.\n",
    "\n",
    "Covariance is calculated using the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "cov(x,y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} \n",
    "\\end{equation}\n",
    "\n",
    "$x$ = predictor variable, $\\bar{x}$ = mean of predictor variable, $y$ = outcome variable, $\\bar{y}$ = mean of outcome variable, $n$ = sample size\n",
    "\n",
    "#### Question 4: What is the covariance of a variable with itself?\n",
    "#### Question 5: What are the units of covariance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. As before, it doesn't necessarily make sense to take the covariance of a variable just because it has numeric values, e.g. PassengerId.\n",
    "\n",
    "2. Covariance units depend on the variables being compared and may not be that easily interpretable.\n",
    "\n",
    "3. \"Large\" covariance is relative to the variable values themselves.\n",
    "\n",
    "4. A variable's covariance with itself is just its variance (squared standard deviation), e.g. for Fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# covariance of Fare with itself\n",
    "print(titanic[\"Fare\"].cov(titanic.Fare))\n",
    "\n",
    "# and variance of Fare\n",
    "print(titanic[\"Fare\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "Unlike covariance, correlation is unitless and always between -1 (perfectly anti-correlated) and 1 (perfectly correlated). Correlation is basically covariance scaled by the respective standard deviations of each variable, which acts to normalize the values and leads to no units. This can make it easier to interpret. \n",
    "\n",
    "The equation for correlation is:\n",
    "\n",
    "\\begin{equation}\n",
    "cor(x,y) = \\frac{cov(x,y)}{std(x)std(y)}\n",
    "\\end{equation}\n",
    "\n",
    "cov(x,y) is the covariance of the predictor and outcome, as before. $std$ is the standard deviation. \n",
    "\n",
    "If correlation is 0, there is no measureable relationship between the variables. Correlation is sensitive to outliers.\n",
    "\n",
    "#### Question 6: What is the correlation of a variable with itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpson's Paradox and Causation\n",
    "\n",
    "Sometimes correlation between variables can be misleading because of the presence of a confounding variable. This means that the _cause_ of variations in the two variables might actually be a third variable. Computing correlation assumes that all other variables are equally affecting the two variables you're comparing, which usually isn't true unless you've got a randomized experiment.\n",
    "\n",
    "For example, there is a 95.86% (0.9586) correlation between per capita mozzarella cheese consumption and civil engineering PhDs awarded between 2000 and 2009. Chances are good that eating more mozzarella will not itself lead to more civil engineering PhDs in the world. There must be a missing variable linking the two.\n",
    "\n",
    "<img src=images/mozzarellaPhDs.png align=\"center\" width=\"75%\"></div>\n",
    "\n",
    "Another classic example is the positive correlation between murder and icecream consumption. Does eating more icecream lead to murderous tendencies? Does murdering someone lead to an icecream craving? Actually, both are independently correlated to higher outside temperatures.\n",
    "\n",
    "It's always good to consider whether a correlation can be explained logically.\n",
    "\n",
    "See more examples:\n",
    "\n",
    "http://www.tylervigen.com/spurious-correlations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 14: Simple Linear Regression\n",
    "\n",
    "Linear regression is a way of computing a model that describes the linear relationship between variables. Simple linear regression uses only one predictor variable while multiple linear regression can include more predictors (also known as features) at once.\n",
    "\n",
    "### Basics\n",
    "\n",
    "The basic idea is that you want to find the line that best describes the relationship between a predictor (e.g. household income) and outcome (e.g. home value). Using the best fit line you calculate, you can predict the outcome for values of the predictor variable that may not be in your dataset.\n",
    "\n",
    "You may know the equation of a line as:\n",
    "\n",
    "$$ y=mx+b $$\n",
    "\n",
    "In linear regression this is expressed as:\n",
    "\n",
    "$$ y = \\beta x + \\alpha + \\epsilon $$\n",
    "\n",
    "$\\beta$ is the slope of the line, $\\alpha$ is the y-intercept, and $\\epsilon$ describes the noise of your data around the line. $\\beta$ tells you how much the outcome value increases for every unit increase in the predictor. $\\alpha$ tells you what the outcome would be even if the predictor is 0. $\\epsilon$ should be small assuming your data is actually linear (e.g. if there was high linear correlation between variables).\n",
    "\n",
    "If you know $\\alpha$ and $\\beta$, you know the line that best estimates your data. In other words, the line is a model of your data!\n",
    "\n",
    "\n",
    "\n",
    "### Finding the Best Fit Line using Least Squares\n",
    "\n",
    "Values for $\\alpha$ and $\\beta$ (which define the regression line) are found using a least squares approach, illustrated below. In effect, the slope and intercept of the line are wiggled around until the offset between the data and the line are minimized. \n",
    "\n",
    "The distance between the data points and the line is measured by the \"sum of squared errors\", literally adding up the area of the squares formed from the difference between the measured outcome (from data) and predicted outcome (from estimated line). Whichever line (i.e. whichever values of $\\alpha$ and $\\beta$) gives the smallest sum of squared errors is the \"least squares estimate\", also known as the line of best fit. It's the best way to describe your linear data.\n",
    "\n",
    "<img src=images/leastsq.gif align=\"center\" width=\"75%\"></div>\n",
    "Source: dynamicgeometry.com\n",
    "\n",
    "#### Question 7: Why is the titanic dataset not useful for practicing linear regression?\n",
    "\n",
    "### When you should use simple linear regression\n",
    "The titanic dataset isn't a good example for doing simple linear regression. To use this method we want a predictor and outcome which both have continuous values (rather than categorical like survival) and are linearly related to each other. \n",
    "\n",
    "Instead of titanic, we'll use a dataset of county data from the U.S. census. It was downloaded from here on 22 June 2016:\n",
    "https://www.openintro.org/stat/data/cc.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "county = pd.read_csv(\"data/countydata.txt\",delimiter='\\t')\n",
    "county.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8: How would you check to see if this data needs to be cleaned before we work with it? (Hint: Look for missing values, unfortunate column names, etc.)\n",
    "\n",
    "#### Question 9: How do you know this dataset is suitable for simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could consider the relationship between a number of variables here. For simple linear regression, we'll consider only one predictor and one outcome. To decide which to use, it might be useful to look at the correlation between sets of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Correlation between per capita income and median value of owner-occupied homes is strongly positively correlated\n",
    "print(county[\"per_capita_income\"].corr(county.median_val_owner_occupied))\n",
    "# Correlation between per capita income and poverty level is strongly negatively correlated.\n",
    "print(county[\"per_capita_income\"].corr(county.poverty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10: Try finding other combinations of variables on which you would want to try simple linear regression.\n",
    "\n",
    "We'll work with just the first example, where higher per capita income is correlated with higher median home values. Feel free to try this method with your own example as well.\n",
    "\n",
    "First, we can visually take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the data\n",
    "sns.regplot('per_capita_income', 'median_val_owner_occupied', \n",
    "           data=county, \n",
    "           fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the positive correlation in the plot. \n",
    "\n",
    "Spoiler alert: seaborn will calculate and plot the regression line without any effort by just turning on the fit_reg flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot with regression line\n",
    "sns.regplot('per_capita_income', 'median_val_owner_occupied', \n",
    "           data=county, \n",
    "           fit_reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it won't show you how good the regression line is or tell you what $\\alpha$ and $\\beta$ are so we can use them to predict specific estimates. But calculating the equation is also easy using the statsmodels function ols (ordinary least squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate the linear model\n",
    "#the regression model is expressed as outcome ~ predictor\n",
    "lm=smf.ols(formula='median_val_owner_occupied ~ per_capita_income', data=county).fit()\n",
    "\n",
    "#print the coefficient values, alpha and beta\n",
    "print(lm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11: The y-intercept, alpha, is negative! What does it mean to have a negative home value? \n",
    "\n",
    "It's reasonable to assume _any_ house will have a value of at least \\$0. And we see this in the data: there are no data points for home values less than \\$0. So for negative home values, our line is an extrapolation and doesn't make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the minimum home value in the data\n",
    "county[\"median_val_owner_occupied\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, consider the case of our lowest possible home value, \\$0. We can find the minimum per capita income that corresponds to the minimum home value by plugging in \\$0 for the home value:\n",
    "\n",
    "$$ homeValue = \\beta * income - \\alpha $$\n",
    "\n",
    "$$ 0 = 12.15 * income - 140,340 $$\n",
    "\n",
    "$$ income = $11,550.69 $$\n",
    "\n",
    "This means we estimate that homeowners in this census dataset make a minimum per capita income of \\$11,550.69 to own even the cheapest house. Of course, this is only an estimate and won't agree with the data perfectly. (Though why is any house listed at $0 value? Perhaps this is mis-reported or missing data?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# actual per capita income corresponding to home value of $0 in the dataset\n",
    "county.per_capita_income[(county.median_val_owner_occupied == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function will go the other way and give us our estimated home value for any income:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm.predict({'per_capita_income':[11550]}) #should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12: Try finding median home value estimates for your own choice of per capita income\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13: Our model estimates that for per capita income of \\$75,000, median home value is \\$771,423. Why might you be skeptical of this estimate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predicted home value for per capita income of $75,000\n",
    "print(lm.predict({'per_capita_income':[75000]})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plug in any number you choose for income, but be wary of out-of-sample estimates. That is, our model has only been evaluated for the range of incomes covered by our data. The same model may not be appropriate for values of income outside this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#range of income in the data\n",
    "print(county[\"per_capita_income\"].min())\n",
    "print(county[\"per_capita_income\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the goodness of your model fit\n",
    "To tell how well our line describes the data, we look at the $R^2$ value. $R^2$ is known as the _coefficient of determination_. It measures how much of the variance in the outcome variable (i.e. home value) is described by the model. $R^2$ ranges from 0 to 1 with a good fit being close to 1. In essence, a high R$^2$ values means that the predictor variable includes enough information to do a good job predicting what's going on with the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print the R^2 value\n",
    "print(lm.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $R^2$ value for our income-home value data is relatively low. It tells us only 55% of the variance in median home value is explained by per capita income. This could suggest additional variables need to be included to better predict median home values. This is a job for multiple linear regression which we'll discuss in a future session.\n",
    "\n",
    "Note the ols calculation also includes a lot more information for more complex statistical analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment for a lot more info from OLS calculation\n",
    "#print(lm.summary()) \n",
    "\n",
    "#uncomment for a complete list of all the quantities of interest you can reference individually\n",
    "#dir(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "One tricky thing about using statistics correctly is to acknowledge and respect their limitations. Every statistical approach will have associated assumptions. Simple linear regression has four:\n",
    "\n",
    "1. Linearity: the data should be best described by a line (instead of a curve, etc.)\n",
    "    * Check with residual plot, should get random distribution around _y = 0_    \n",
    "2. Homoscedasticity: variance should be constant for range of predictor values\n",
    "    * Check with same residual plot, spread of residuals shouldn't get larger as a function of predictor value\n",
    "3. Normality: errors should be normally distributed.\n",
    "    * Check with Q-Q plot, should get the identity line\n",
    "4. No autocorrelation of errors: each observation should be independent of the others\n",
    "    * Check with residual plot, there shouldn't be a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check for linearity\n",
    "sns.residplot(county.median_val_owner_occupied,lm.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks fine for linearity. Nonlinear data would show residuals with some curvature around the 0 line like this:\n",
    "\n",
    "<img src=images/Nonlinear-residual.png align=\"center\" width=\"50%\"></div>\n",
    "\n",
    "Source: http://docs.statwing.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for constant variance (homoscedasticity)\n",
    "sns.residplot(county.median_val_owner_occupied,lm.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spread about the 0 residual line doesn't seem to change all that much with home value, so the data appears homoscedastic. Heteroscedastic data would look something like this:\n",
    "\n",
    "\n",
    "<img src=images/heteroskedastic.png align=\"center\" width=\"50%\"></div>\n",
    "\n",
    "Source: http://www.econometricsbysimulation.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check for normality\n",
    "fig = plt.figure()\n",
    "fig=smg.qqplot(lm.resid,fit=True, line='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape of QQ plot is not that uncommon. The data is normal in the center of the distribution but our dataset has values more exteme than our model predicts. Because most of the data falls on the line, this isn't too poorly behaved, but note that the distribution isn't quite normal because of the thicker tails, particular at the higher end of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check for error independence\n",
    "#pd.tools.plotting.autocorrelation_plot(county)\n",
    "#sns.regplot('per_capita_income',lm.resid,\n",
    "#           data=county, \n",
    "#           fit_reg=False)\n",
    "\n",
    "sns.residplot(county.median_val_owner_occupied,lm.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We check for independence in the _errors_, not the outcome variable itself. This data looks to have uncorrelated errors, because they seem to vary randomly. \n",
    "\n",
    "When errors are correlated, each one will appear to follow from the last in a pattern.\n",
    "\n",
    "\n",
    "<img src=images/correlatedErrors.png align=\"center\" width=\"50%\"></div>\n",
    "\n",
    "Source: http://zoonek2.free.fr/UNIX/48_R/11.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
